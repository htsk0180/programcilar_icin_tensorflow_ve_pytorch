{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Better Transformer (BT)"
      ],
      "metadata": {
        "id": "IbH6BlWSZkfT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch 1.12 sürümünün bir parçası olarak Better Transformer (BT) tanıtılmaktadır.Better Transformer, CPU ve GPU'da yüksek performansla Transformer modellerinin dağıtımını hızlandırmak için üretime hazır bir hızlı yoldur. Fastpath özelliği, doğrudan PyTorch core nn.module veya torchtext tabanlı modeller için şeffaf bir şekilde çalışır.TransformerEncoder, TransformerEncoderLayer ve MultiHeadAttention kullanarak bu işlemleri yapar."
      ],
      "metadata": {
        "id": "VqsMxDVaZHRh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ön Hazırlıklar"
      ],
      "metadata": {
        "id": "dJbad0VWaqTZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Better Transformer iki tür hızlandırma sunar:\n",
        "\n",
        "\n",
        "1.   Genel yürütme verimliliğini artırmak amacıyla CPU ve GPU için yerel multihead attention (MHA) mekanizmasını uygular.\n",
        "2.   NLP çıkarımında seyrekliği kullanmak. Değişken girdi uzunlukları nedeniyle, girdi belirteçleri, işlemenin atlanabileceği çok sayıda dolgu belirteci içerebilir ve bu da önemli hızlanmalar sağlar.\n",
        "\n",
        "Fastpath execution bazı kriterlere tabidir. En önemlisi, modelin çıkarım modunda çalıştırılması ve gradyan bilgilerini toplamayan( örneğin with torch.no_grad modu ) giriş tensörleri üzerinde çalışması gerekir.\n",
        "\n"
      ],
      "metadata": {
        "id": "gyi8pw2lbgus"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Şunların testini ve tespitini yapacağız.\n",
        "\n",
        "\n",
        "\n",
        "*   Önceden eğitilmiş modelleri yükleyin (Better Transformer olmadan oluşturulan 1.12 version ve öncesi)\n",
        "*   BT fastpath yolu olan ve olmayan CPU'da çıkarım çalıştırma ve kıyaslama karşılaştırması (yalnızca yerel MHA)\n",
        "*   BT fastpath yolu olan ve olmayan yapılandırılabilir) CİHAZ üzerinde çıkarım çalıştırın ve kıyaslama yapın (yalnızca yerel MHA)\n",
        "*   Sparsity /Seyreklik desteğini etkinleştir\n",
        "*   BT fastpath yolu olan ve olmayan (yerel MHA + seyreklik) (yapılandırılabilir) CİHAZ üzerinde çıkarım çalıştırın ve kıyaslama yapın\n",
        "\n",
        "Daha fazla bilgi için : https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference//\n"
      ],
      "metadata": {
        "id": "-_RJgiIdcSRt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.Önceden eğitilmiş modelleri yükleyin."
      ],
      "metadata": {
        "id": "C5rQYidzdQrP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "XLM-R modelini, torchtext.models içerisindeki yönergeleri takip ederek önceden tanımlı torchtext modellerinden indiriyoruz. Cihazı ayrıca hızlandırıcı üzerinde testler yapacak şekilde ayarladık. (Ortamınız için uygun şekilde GPU yürütmeyi etkinleştirin.)"
      ],
      "metadata": {
        "id": "GIvsuwNEdWqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "print(f\"torch version: {torch.__version__}\")\n",
        "\n",
        "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "print(f\"torch cuda available: {torch.cuda.is_available()}\")\n",
        "\n",
        "import torch, torchtext\n",
        "from torchtext.models import RobertaClassificationHead\n",
        "from torchtext.functional import to_tensor\n",
        "xlmr_large = torchtext.models.XLMR_LARGE_ENCODER\n",
        "classifier_head = torchtext.models.RobertaClassificationHead(num_classes=2, input_dim = 1024)\n",
        "model = xlmr_large.get_model(head=classifier_head)\n",
        "transform = xlmr_large.transform()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eGsLK86ZjtI",
        "outputId": "d5be16d3-cfb1-425d-9b1a-eb369b34f4ea"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch version: 1.13.1+cu116\n",
            "torch cuda available: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.Data Setup"
      ],
      "metadata": {
        "id": "wwaTRRRkaiNz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "İki tür girdi oluşturduk: küçük bir girdi grubu ve seyrekliğe sahip büyük bir girdi grubu."
      ],
      "metadata": {
        "id": "b7jwsvhydsql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "small_input_batch = [\n",
        "               \"Hello world\",\n",
        "               \"How are you!\"\n",
        "]\n",
        "big_input_batch = [\n",
        "               \"Hello world\",\n",
        "               \"How are you!\",\n",
        "               \"\"\"`Well, Prince, so Genoa and Lucca are now just family estates of the\n",
        "Buonapartes. But I warn you, if you don't tell me that this means war,\n",
        "if you still try to defend the infamies and horrors perpetrated by\n",
        "that Antichrist- I really believe he is Antichrist- I will have\n",
        "nothing more to do with you and you are no longer my friend, no longer\n",
        "my 'faithful slave,' as you call yourself! But how do you do? I see\n",
        "I have frightened you- sit down and tell me all the news.`\n",
        "\n",
        "It was in July, 1805, and the speaker was the well-known Anna\n",
        "Pavlovna Scherer, maid of honor and favorite of the Empress Marya\n",
        "Fedorovna. With these words she greeted Prince Vasili Kuragin, a man\n",
        "of high rank and importance, who was the first to arrive at her\n",
        "reception. Anna Pavlovna had had a cough for some days. She was, as\n",
        "she said, suffering from la grippe; grippe being then a new word in\n",
        "St. Petersburg, used only by the elite.\"\"\"\n",
        "]"
      ],
      "metadata": {
        "id": "nTCL634HanGq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ardından, küçük veya büyük girdi yığınını seçer, girdileri önceden işler ve modeli test ederiz."
      ],
      "metadata": {
        "id": "3rIPkKwvdxqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_batch=big_input_batch\n",
        "\n",
        "model_input = to_tensor(transform(input_batch), padding_value=1)\n",
        "output = model(model_input)\n",
        "output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sq6209dLdx5y",
        "outputId": "2167af16-42f4-4674-f55b-a70ed8bf691a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Son olarak, kıyaslama yineleme sayısını belirledik"
      ],
      "metadata": {
        "id": "_pAutl_nd4tX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ITERATIONS=10"
      ],
      "metadata": {
        "id": "eYKZ8f68d5qw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.BT fastpath yolu olan ve olmayan CPU'da çıkarım çalıştırma ve kıyaslama karşılaştırması (yalnızca yerel MHA)"
      ],
      "metadata": {
        "id": "pJXNaYgXeCgU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "İlk çalıştırma, geleneksel yürütmeyi kullanır.\n",
        "\n",
        "İkinci çalıştırma, modeli model.eval() kullanarak çıkarım moduna getirerek BT fastpath yürütmesini etkinleştirir ve torch.no_grad() ile gradyan toplamayı devre dışı bırakır.\n",
        "\n",
        "Model CPU üzerinde yürütülürken (büyüklüğü CPU modeline bağlı olacaktır) bir gelişme görebilirsiniz. Fastpath profilinin yerel TransformerEncoderLayer uygulamasında yürütme süresinin çoğunu transformer_encoder_layer_fwd gösterdiğine dikkat edin."
      ],
      "metadata": {
        "id": "COsf0mVXeSnh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"slow path:\")\n",
        "print(\"==========\")\n",
        "with torch.autograd.profiler.profile(use_cuda=False) as prof:\n",
        "  for i in range(ITERATIONS):\n",
        "    output = model(model_input)\n",
        "print(prof)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "print(\"fast path:\")\n",
        "print(\"==========\")\n",
        "with torch.autograd.profiler.profile(use_cuda=False) as prof:\n",
        "  with torch.no_grad():\n",
        "    for i in range(ITERATIONS):\n",
        "      output = model(model_input)\n",
        "print(prof)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7JvDFHiemNV",
        "outputId": "1e722d66-c812-487a-f834-bf3e95870b12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "slow path:\n",
            "==========\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.BT fastpath yolu olan ve olmayan (yapılandırılabilir) cihaz üzerinde çalıştırma ve kıyaslama çıkarımı (yalnızca yerel MHA)"
      ],
      "metadata": {
        "id": "kpW7suuge72S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " BT sparsity ayarlarını kontrol edeceğiz."
      ],
      "metadata": {
        "id": "WRgHCqNffHj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.encoder.transformer.layers.enable_nested_tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbTJH8RWfJaY",
        "outputId": "af991c21-db0c-4a48-ee18-4a5f40981b76"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Şimdi BT sparsity ayarlarını devre dışı bırakalım."
      ],
      "metadata": {
        "id": "Qu8ngekXfgv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.encoder.transformer.layers.enable_nested_tensor=False"
      ],
      "metadata": {
        "id": "HDUiHlbIfmX7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modeli cihaz üzerinde çalıştırıyoruz ve cihaz üzerinde yerel MHA yürütmesi için profil bilgilerini topluyoruz:\n",
        "\n",
        "İlk çalıştırma, geleneksel yürütmeyi kullanır.\n",
        "\n",
        "İkinci çalıştırma, modeli model.eval() kullanarak çıkarım moduna getirerek BT fastpath yürütmesini etkinleştirir ve torch.no_grad() ile gradyan toplamayı devre dışı bırakır.\n",
        "\n",
        "Bir GPU'da çalıştırırken, özellikle küçük girdi grubu ayarı için önemli bir hızlanma görmelisiniz."
      ],
      "metadata": {
        "id": "IRlHfZFCfwLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(DEVICE)\n",
        "model_input = model_input.to(DEVICE)\n",
        "\n",
        "print(\"slow path:\")\n",
        "print(\"==========\")\n",
        "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
        "  for i in range(ITERATIONS):\n",
        "    output = model(model_input)\n",
        "print(prof)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "print(\"fast path:\")\n",
        "print(\"==========\")\n",
        "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
        "  with torch.no_grad():\n",
        "    for i in range(ITERATIONS):\n",
        "      output = model(model_input)\n",
        "print(prof)"
      ],
      "metadata": {
        "id": "YWNvKILcf5FD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.BT fastpath yolu olan veya olmayan (yerel MHA + BT sparsity) (yapılandırılabilir) cihaz üzerinde çıkarımı"
      ],
      "metadata": {
        "id": "7ZAW93epgDug"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BT sparsity desteğini etkinleştiriyoruz."
      ],
      "metadata": {
        "id": "qBAB6VgPgavN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.encoder.transformer.layers.enable_nested_tensor = True"
      ],
      "metadata": {
        "id": "Hyo3cHG6gd7D"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modeli cihaz üzerinde çalıştırıyoruz ve cihaz üzerinde yerel MHA ve sparsity desteği yürütmesi için profil bilgilerini topluyoruz:\n",
        "\n",
        "İlk çalıştırma, geleneksel yürütmeyi kullanır.\n",
        "\n",
        "İkinci çalıştırma, modeli model.eval() kullanarak çıkarım moduna getirerek BT hızlı yol yürütmesini etkinleştirir ve torch.no_grad() ile gradyan toplamayı devre dışı bırakır.\n",
        "\n",
        "Bir GPU'da çalıştırırken, özellikle seyreklik içeren büyük toplu girdi ayarı için önemli bir hızlanma görmelisiniz"
      ],
      "metadata": {
        "id": "cdIsgo71gknJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(DEVICE)\n",
        "model_input = model_input.to(DEVICE)\n",
        "\n",
        "print(\"slow path:\")\n",
        "print(\"==========\")\n",
        "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
        "  for i in range(ITERATIONS):\n",
        "    output = model(model_input)\n",
        "print(prof)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "print(\"fast path:\")\n",
        "print(\"==========\")\n",
        "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
        "  with torch.no_grad():\n",
        "    for i in range(ITERATIONS):\n",
        "      output = model(model_input)\n",
        "print(prof)"
      ],
      "metadata": {
        "id": "XvbHAHMfgudy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Özet"
      ],
      "metadata": {
        "id": "AwSzRMQ3g1Zz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bu eğitimde, Transformer Encoder modelleri için PyTorch çekirdeği Better Transformer desteğini kullanarak torch metninde Better Transformer hızlı yol yürütme ile hızlı transformatör çıkarımını tanıttık. Better Transformer'ın kullanımını, BT fastpath yol yürütmesinin kullanıma sunulmasından önce eğitilmiş modellerle gösterdik. Hem BT fastpath yürütme modlarının, yerel MHA yürütmenin hem de BT sparsity hızlandırmasının kullanımını gösterdik ve kıyasladık."
      ],
      "metadata": {
        "id": "q1_s6CS1g3l7"
      }
    }
  ]
}