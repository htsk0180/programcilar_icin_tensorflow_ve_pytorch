{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Numpy"
      ],
      "metadata": {
        "id": "V6iaBBIV4Ckv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CQ-RbEq1Zgo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# random giriş ve random çıkışlarımızı üretelim.\n",
        "\n",
        "x = np.linspace(-math.pi , math.pi, 200)\n",
        "y = np.sin(x)"
      ],
      "metadata": {
        "id": "dZk4SDex1ot5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sinir ağımızın ağırklıklarını random olarak başlatalım.\n",
        "\n",
        "a = np.random.randn()\n",
        "b = np.random.randn()\n",
        "c = np.random.randn()\n",
        "d = np.random.randn()"
      ],
      "metadata": {
        "id": "TuSsYC8C14DV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# learning rate ve eğitim döngümüzü ayarlayalım.\n",
        "\n",
        "learning_rate = 1e-6\n",
        "epochs = 2000\n",
        "for t in range(epochs):\n",
        "  #forward: tahmin edilen y'yi hesaplayalım\n",
        "  # y = a + b x + c x^2 + d x^3\n",
        "  y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
        "\n",
        "  # tahmin ve gerçek değer arasındaki kaybı hesaplayalım\n",
        "  loss = np.square(y_pred - y).sum()\n",
        "  if t % 100 == 99:\n",
        "    print(t, loss)\n",
        "\n",
        "  # backward : kayba göre a, b, c, d gradyanlarını hesaplamak için Backprop yazalım\n",
        "\n",
        "  grad_y_pred = 2.0 * (y_pred - y)\n",
        "  grad_a = grad_y_pred.sum()\n",
        "  grad_b = (grad_y_pred * x).sum()\n",
        "  grad_c = (grad_y_pred * x ** 2).sum()\n",
        "  grad_d = (grad_y_pred * x ** 3).sum()\n",
        "\n",
        "  # hesaplanan gradyanlar ve learning rate kullanrarak ağırlıkları güncelleyelim.\n",
        "  a -= learning_rate * grad_a\n",
        "  b -= learning_rate * grad_b\n",
        "  c -= learning_rate * grad_c\n",
        "  d -= learning_rate * grad_d\n",
        "\n",
        "print(f'sonuçlar: y = {a} + {b} x + {c} x^2 + {d} x^3')\n"
      ],
      "metadata": {
        "id": "SObQ86sR2UWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tensor"
      ],
      "metadata": {
        "id": "4TFXD7iE4HYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# random şekilde giriş ve çıkış değerlerini oluşturalım.\n",
        "x = np.linspace(-math.pi, math.pi, 2000)\n",
        "y = np.sin(x)\n",
        "\n",
        "# ağırlıkları random olarak başlatalım.\n",
        "a = np.random.randn()\n",
        "b = np.random.randn()\n",
        "c = np.random.randn()\n",
        "d = np.random.randn()\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(2000):\n",
        "    #forward: tahmin edilen y'yi hesaplayalım\n",
        "    # y = a + b x + c x^2 + d x^3\n",
        "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
        "\n",
        "    # kaybı hesaplayalım.\n",
        "    loss = np.square(y_pred - y).sum()\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss)\n",
        "\n",
        "    # backward : kayba göre a, b, c, d gradyanlarını hesaplamak için Backprop yazalım\n",
        "    grad_y_pred = 2.0 * (y_pred - y)\n",
        "    grad_a = grad_y_pred.sum()\n",
        "    grad_b = (grad_y_pred * x).sum()\n",
        "    grad_c = (grad_y_pred * x ** 2).sum()\n",
        "    grad_d = (grad_y_pred * x ** 3).sum()\n",
        "\n",
        "    # ağırlıkları güncelleyelim.\n",
        "    a -= learning_rate * grad_a\n",
        "    b -= learning_rate * grad_b\n",
        "    c -= learning_rate * grad_c\n",
        "    d -= learning_rate * grad_d\n",
        "\n",
        "print(f'Sonuçlar: y = {a} + {b} x + {c} x^2 + {d} x^3')"
      ],
      "metadata": {
        "id": "H7BSTUoS4I03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yukarıdaki örneklerimizde sinir ağımızın hem ileri hemde geri geçişlerini manuel olarak uyguladık. Özellikle geriye yayılımı manuel olarak uygulamak küçük networklerde sorun değil ancak büyük ağ mimarilerinde çok sorun olacaktır.\n",
        "\n",
        "> Bu problemi çözebilmek adına pytorch da ki autograd paketi bu işlemi bizim için yapar.\n",
        "\n"
      ],
      "metadata": {
        "id": "UwqnDEg4460X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# nn module"
      ],
      "metadata": {
        "id": "6qwh6fac59RJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import torch\n",
        "import math\n",
        "\n",
        "\n",
        "# giriş ve çıkış değerlerimizi ayarlayalım.\n",
        "x = torch.linspace(-math.pi, math.pi, 2000)\n",
        "y = torch.sin(x)\n",
        "\n",
        "# örnekte y çıkışı doğrusal bir fonksiyondur. bu nedenle bunu doğrusal katmanlı\n",
        "# bir sinir ağı olarak düşünebiliriz.\n",
        "p = torch.tensor([1, 2, 3])\n",
        "xx = x.unsqueeze(-1).pow(p)\n",
        "\n",
        "\n",
        "# modelimizi katmanlar dizisi olarak tanımlamak için: nn.Sequential\n",
        "# nn.Sequential diğer modülleri içeren ve bunları sırayla uygulayan bir modüldür.\n",
        "# modül giriş değerinden çıkış değerlerini üretirken ağırlık ve bias değerini\n",
        "# içeren tensörleri tutar.\n",
        "# flatten layer doğrusal katmanın çıktısını 1D tensörüne çevirir.\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(3, 1),\n",
        "    torch.nn.Flatten(0, 1)\n",
        ")\n",
        "\n",
        "# nn paketi ayrıca popüler kayıp işlevlerini de barındırır.\n",
        "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(2000):\n",
        "\n",
        "    # forward geçişi sayesinde giriş olarak verilen tensörlerden\n",
        "    # çıkış tensörleri üretilir.\n",
        "    y_pred = model(xx)\n",
        "\n",
        "    # aynı şekilde loss ve y değerleri bir tensördür.\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss.item())\n",
        "\n",
        "    # backpro' yu çalıştırnadan önceden gradyanları sıfırlayalım.\n",
        "    model.zero_grad()\n",
        "\n",
        "    # Backward pass: kayba göre gradyanları hesaplıyoruz.\n",
        "    # requires_grad=True olduğunda tüm öğrenilebilir parametreler için kayba göre\n",
        "    # gradyanlar hesaplanır.\n",
        "    # her modülün parametreleri saklanır.\n",
        "    loss.backward()\n",
        "\n",
        "    # gradyan iniş kullanılarak parametreler güncellenir. her parametre bir \n",
        "    # tensördür.\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for param in model.parameters():\n",
        "            param -= learning_rate * param.grad\n",
        "\n",
        "# Bir listenin ilk öğesine erişmek gibi `modelin' ilk katmanına erişebilirsiniz.\n",
        "linear_layer = model[0]\n",
        "\n",
        "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMiUoJ1D6BLu",
        "outputId": "1454e8db-c7cc-4bdb-be83-8764673d033a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99 1358.1866455078125\n",
            "199 905.281494140625\n",
            "299 604.5720825195312\n",
            "399 404.86029052734375\n",
            "499 272.1868591308594\n",
            "599 184.0220947265625\n",
            "699 125.41560363769531\n",
            "799 86.44464111328125\n",
            "899 60.52105712890625\n",
            "999 43.27019119262695\n",
            "1099 31.785930633544922\n",
            "1199 24.13736343383789\n",
            "1299 19.04107666015625\n",
            "1399 15.643819808959961\n",
            "1499 13.378094673156738\n",
            "1599 11.866195678710938\n",
            "1699 10.856791496276855\n",
            "1799 10.182450294494629\n",
            "1899 9.731709480285645\n",
            "1999 9.430209159851074\n",
            "Result: y = -0.011912121437489986 + 0.8353021740913391 x + 0.002055038930848241 x^2 + -0.0902809202671051 x^3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# optim"
      ],
      "metadata": {
        "id": "6aXwl7dG_c3r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bu noktaya kadar, öğrenilebilir parametreleri tutan Tensörleri **torch.no_grad()** ile manuel olarak değiştirerek modellerimizin ağırlıklarını güncelledik. Bu, stokastik gradyan inişi gibi basit optimizasyon algoritmaları için büyük bir yük değildir\n",
        "\n",
        "\n",
        "> **optim paketi**, bir optimizasyon algoritması fikrini özetler ve yaygın olarak kullanılan optimizasyon algoritmalarının uygulamalarını sağlar.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VMRJIV_8_pLw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FgiTT93M_9wS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}